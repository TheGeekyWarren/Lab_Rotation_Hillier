{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import deeplabcut\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_file  = '/home/yramakrishna/DeepLabCut/conda-environments/Codes/Ken_Test_Long.mp4' #loading path of the original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42328f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsv = deeplabcut.DownSampleVideo(start_file, width=320, height=240) # the video is being downsampled and its path is stored in variable dsv, with resolution of 320X240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained face detection model using Open CV's dnn module, where deploy.prototxt.txt has the network architecture\n",
    "#and res10_300x300_ssd_iter_140000_fp16.caffemodel contains the trained weights\n",
    "model = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(dsv) #capturing the video filoe for processing\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))#initialising variable for frames per rate. It extracts the frame rate of the video using cap.get\n",
    "\n",
    "# Set the video writer, named CropGR, its codec, fps and frame size in pixels is being specified\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') #codec of the output video\n",
    "out = cv2.VideoWriter('CropGr.mp4', fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "check = False\n",
    "frame_lum_val = []# initialising an empty list to store luminence value of each frame\n",
    "target_br = 0.6 # setting the traget brightness(can range from 0 to 1)\n",
    "target_g = math.log(target_br)# calculating the corresponding gamma value based on the brightness\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)# Setting the current frame position of the cv2.VideoCapture \n",
    "#object to the beginning of the video file (the first frame).\n",
    "\n",
    "# Loop through the frames in the video\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()#Read the next frame from the video and store it in the variable frame. \n",
    "    # boolean variable ret indicates whether the frame was successfully read.\n",
    "    check = False\n",
    "    \n",
    "    # Check if the frame was successfully read\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Resize the frame to a smaller size (for faster processing)\n",
    "    small_frame = cv2.resize(frame, (300, 300))\n",
    "    \n",
    "    # Preprocess the frame for input to the neural network\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0)) ????\n",
    "    \n",
    "    # Pass the blob through the neural network to detect faces\n",
    "    model.setInput(blob)\n",
    "    detections = model.forward()\n",
    "    #These lines pass the image blob through the pre-trained face detection model loaded in the previous line to detect faces in the frame. The setInput method sets the input blob to the network, \n",
    "    #and forward method performs a forward pass of the network on the input and returns the output detections.\n",
    "    \n",
    "     # Loop through the detections and draw rectangles around the monkey faces\n",
    "    for i in range(detections.shape[2]): ???logic\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.7:  # Only show faces with high confidence+\n",
    "            check = True\n",
    "            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            #identify region around face and perform adaptive gamma correction\n",
    "            subframe = frame[startX:endX, startY:endY]\n",
    "            subframe_normalized = subframe.astype(np.float32) / 255.0   #First, the subframe around the face is extracted from the frame and normalized to the range of 0 to 1.\n",
    "            gray = cv2.cvtColor(subframe_normalized, cv2.COLOR_BGR2GRAY) #converted to gray scale to get brightness\n",
    "            #The brightness of the subframe is computed by converting it to grayscale and taking the mean value.\n",
    "            brightness = np.mean(gray)\n",
    "            \n",
    "            #finding and correcting gamma - a number is assigned to be the 'innate' gamma value of the frame, and that value is used to find the correction needed. \n",
    "            assigned_g = math.log(brightness)\n",
    "            gamma = assigned_g/target_g\n",
    "            # #This brightness value is used to compute the gamma correction factor by taking the natural logarithm of the brightness value\n",
    "            #and dividing it by the target brightness value.\n",
    "            \n",
    "            #implementing gamma correction\n",
    "            frame_normalized = frame.astype(np.float32) / 255.0\n",
    "            \n",
    "            frame_corrected = np.power(frame_normalized, 1/gamma)\n",
    "            \n",
    "            frame_scaled = (frame_corrected * 255.0).astype(np.uint8)\n",
    "            \n",
    "            # cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            \n",
    "#In this part of the code, the detected faces from the previous part are used to perform gamma correction on the video frames.\n",
    "\n",
    "#First, for each detected face, the confidence score is checked to ensure that it is above a certain threshold (0.7). If the confidence score is high enough, the region around the face is identified and normalized to the range of 0 to 1.\n",
    "\n",
    "#Next, the average brightness of the region around the face is computed using grayscale conversion. The brightness value is then used to calculate a gamma correction factor that will be applied to the entire frame.\n",
    "\n",
    "#The gamma correction is implemented by normalizing the frame to the range of 0 to 1, applying the gamma correction factor to each pixel value, and then scaling the pixel values back to the range of 0 to 255.\n",
    "\n",
    "#Finally, the corrected frame is written to the output video file, and the loop continues until all frames have been processed. The loop can be interrupted by pressing the 'q' key.\n",
    "\n",
    "            \n",
    "    # cv2.imshow(\"Monkey faces\", frame_scaled)\n",
    "    if check:\n",
    "        out.write(frame_scaled)\n",
    "    \n",
    "    # Wait for a key press\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video file and close the window\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccb272b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21000\\977140396.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"final_video\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CropGr.mp4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'final_video/CropGr.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;34m'final_video/CropGr.mp4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_video'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# dsv = deeplabcut.DownSampleVideo(start_file, width=320, height=240)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"final_video\") \n",
    "#create a new directory named final video\n",
    "\n",
    "shutil.move('CropGr.mp4', 'final_video/CropGr.mp4')\n",
    "#moves the output file to the final video directory\n",
    "\n",
    "os.chdir('final_video')\n",
    "#changes the current working directory to \"final_video\"\n",
    "\n",
    "file = 'CropGr.mp4'\n",
    "bodyparts = ['RightEye_Outer', 'RightEye_Top', 'RightEye_Bottom', 'RightEye_Inner', 'RightEye_Pupil', 'LeftEye_Outer', 'LeftEye_Top', 'LeftEye_Bottom', 'LeftEye_Inner', 'LeftEye_Pupil',\n",
    "             'OutlineTop_Mid','RightNostrils_Top', 'RightNostrils_Bottom','LeftNostrils_Top', 'LeftNostrils_Bottom']\n",
    "videotype = os.path.splitext(file)[-1].lstrip('.')  # or MOV, or avi, whatever you uploaded! extracts the file extension and stores in video type\n",
    "video_down = file# sets this variable to the name of the video file\n",
    "\n",
    "model_options = deeplabcut.create_project.modelzoo.Modeloptions\n",
    "model_selection = 'primate_face' #selects the model zoo's primate face model\n",
    "project_name = 'DLC_GazeXBI'#name of the project\n",
    "your_name = 'anc'#user name\n",
    "\n",
    "config_path, train_config_path = deeplabcut.create_pretrained_project(\n",
    "    project_name,\n",
    "    your_name,\n",
    "    video_down,\n",
    "    videotype=videotype,\n",
    "    model=model_selection,\n",
    "    analyzevideo=True,\n",
    "    createlabeledvideo=False,\n",
    "    copy_videos=True,\n",
    ")\n",
    "\n",
    "#creates a new project with DeepLabCut by calling the create_pretrained_project function, \n",
    "#and returns the configuration file paths and training configuration file paths as variables. \n",
    "#It used the previously mentioned model, project name, and user name. It also specifies \n",
    "#the input video file, the file type, and sets the \"analyzevideo\" and \"copy_videos\" flags to True. \n",
    "#It sets the \"createlabeledvideo\" flag to False to prevent the creation of a labeled video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66251ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionery \n",
    "edits = {\n",
    "    'dotsize': 2,  # size of the dots on the output video!\n",
    "    'pcutoff': 0.4,  # the higher, the more conservative the plotting!\n",
    "}\n",
    "deeplabcut.auxiliaryfunctions.edit_config(config_path, edits)\n",
    "#auxiliary functions is called with the path to the project configuration file (config_path)\n",
    "# and the dictionary of edits as arguments. It updates the project configuration file with the edits.\n",
    "\n",
    "project_path = os.path.dirname(config_path) ??\n",
    "full_video_path = os.path.join(\n",
    "    project_path,\n",
    "    'videos',\n",
    "    os.path.basename(video_down),\n",
    ")\n",
    "\n",
    "# filter predictions (should already be done above ;):\n",
    "deeplabcut.filterpredictions(config_path, full_video_path, videotype=videotype, )\n",
    "#This code calls the filterpredictions function from DeepLabCut with the project configuration file path (config_path), \n",
    "#the full path to the video file (full_video_path), and the video type (videotype) as arguments. This function filters the predictions made by the neural network, \n",
    "#removing any that are below the confidence threshold set in the project configuration file.\n",
    "\n",
    "# re-create the video with your edits!\n",
    "# deeplabcut.CropVideo(config_path, full_video_path, 'crop', \n",
    "\n",
    "deeplabcut.create_labeled_video(config_path, full_video_path, videotype=videotype, displayedbodyparts=bodyparts, draw_skeleton = True, filtered=True)\n",
    "#Calling create_labeled_video function from DeepLabCut with the project configuration file path the full path to the video file , and the video type as arguments. \n",
    "#It also specifies the body parts to display in the output video , sets the draw_skeleton parameter to True to draw lines connecting the body parts, and sets the filtered parameter to True to use the filtered predictions. \n",
    "#This function generates a labeled video file that shows the predicted positions of the body parts in the original video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move('/home/yramakrishna/DeepLabCut/conda-environments/final_video/DLC_GazeXBI-anc-2023-03-28/videos/CropGrDLC_resnet50_DLC_GazeXBIMar28shuffle1_1030000_filtered_labeled.mp4', '/home/yramakrishna/DeepLabCut/conda-environments/CropGrDLC_resnet50_DLC_GazeXBIMar28shuffle1_1030000_filtered_labeled.mp4')\n",
    "#This line moves the labeled video file generated by DeepLabCut from a subdirectory (final_video/DLC_GazeXBI-anc-2023-03-28/videos/) #\n",
    "#to the parent directory (/home/yramakrishna/DeepLabCut/conda-environments/).\n",
    "try:\n",
    "    shutil.rmtree('/home/yramakrishna/DeepLabCut/conda-environments/final_video') #tries to remove final video directory\n",
    "    # If it fails, inform the user.\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "try:\n",
    "    os.remove('/home/yramakrishna/DeepLabCut/conda-environments/Ken_Test_Longdownsampled.mp4')#tries to remove ken but why?\n",
    "except OSError as e:\n",
    "    # If it fails, inform the user.\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf93296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkVideoPlayer import TkinterVideo\n",
    "\n",
    "root = tk.Tk()\n",
    "\n",
    "videoplayer = TkinterVideo(master=root, scaled=True)\n",
    "videoplayer.load(r\"/home/yramakrishna/DeepLabCut/conda-environments/CropGrDLC_resnet50_DLC_GazeXBIMar28shuffle1_1030000_filtered_labeled.mp4\")\n",
    "videoplayer.pack(expand=True, fill=\"both\")\n",
    "\n",
    "videoplayer.play() # play the video\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
